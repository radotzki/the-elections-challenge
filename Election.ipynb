{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import pandas as pd\n",
    "import pylab as p\n",
    "\n",
    "def print_missing_values(df):\n",
    "    print '\\n\\nMissing values:'\n",
    "    for col in df.columns.values:\n",
    "        misCount = df[col].isnull().sum()\n",
    "        if misCount > 0:\n",
    "            print '%s: %s missing values' % (col, misCount)\n",
    "\n",
    "def plot_log_density_function(df, column):\n",
    "    plot_density_function(df, column, lambda x: math.log(x))\n",
    "\n",
    "\n",
    "def plot_density_function(df, column, trans=lambda x: x):\n",
    "    df[column].plot(kind='kde')    \n",
    "    # p.savefig('foo.png')\n",
    "    # p.clf()\n",
    "    p.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features that were uniformly distributed seem insignificant by this test, did we do anything wrong? \n",
    "Lets print their mean by vote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "votes=df.Vote.unique()\n",
    "votes.sort()\n",
    "\n",
    "for c in df.columns:\n",
    "    if c in not_normal:\n",
    "        print c\n",
    "        for vote in votes:                                \n",
    "            print str(vote) + \": \"  + str(df[df.Vote==vote][c].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of them seem meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do next:\n",
    "- find more insights like this -> more meaningful features\n",
    "- Maybe find more tests (we could leave it for later) \n",
    "- Find features that are highly related to each other (same way we found features that are highly related to the label). Then maybe for each group, find the one that is most significant to the label, and use wrapper method to see if removing the others decrease performance of different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing KDE's between classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "votes=df.Vote.unique()\n",
    "votes.sort()\n",
    "\n",
    "for c in ALL_FEATURES:\n",
    "    if c not in FEATURES_TO_KEEP: #only look at features we still have'nt found any reason to keep    \n",
    "        if c not in DISCRETE_FEATURES: # this is only relevant for non-discrete features\n",
    "            print c + \": \" +  str(c in features_to_keep)\n",
    "            for vote in votes:                                \n",
    "                df[df.Vote==vote][c].plot(kind='kde')            \n",
    "            plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: no new evidence to keep one of those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    if c in features_to_keep: # lets see if there are redundant features we decided to keep\n",
    "        if c not in discrete_features: # this is only relevant for non-discrete features\n",
    "            print c + \": \" +  str(c in features_to_keep)\n",
    "            for vote in votes:                                \n",
    "                df[df.Vote==vote][c].plot(kind='kde')            \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#these 2 look the same\n",
    "df.plot(x='Avg_monthly_expense_when_under_age_21', y='Avg_Residancy_Altitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they are linearly dependent, so we can drop one of them, and even use it to fill the other one's missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#these two look pretty similar too\n",
    "df.plot(x='Yearly_ExpensesK', y='Avg_monthly_expense_on_pets_or_plants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if thats close enough to drop one of them, let's keep them for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for vote in votes:\n",
    "    df[df.Vote==vote].plot(y='Yearly_ExpensesK', x='Avg_monthly_expense_on_pets_or_plants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Vote.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for discrete features, lets compare histograms of votes between different values\n",
    "for c in discrete_features:\n",
    "    print c + \": \" +  str(c in features_to_keep)    \n",
    "    ct = pd.crosstab(df[c], df.Vote)\n",
    "    ctn=ct.div(ct.sum(axis=1), axis=0).div(ct.sum(axis=0), axis=1) #normalize\n",
    "    ctn.plot(kind='bar', figsize = (15,5)).legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()    \n",
    "    #lets also print the KDEs, maybe it will help    \n",
    "    if len(df[c].unique())>2:\n",
    "        for vote in votes:                                        \n",
    "            df[df.Vote==vote][c].plot(kind='kde')            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occupation_Satisfaction- looks different for party 6 when value is 4 and 10, BUT 6 is a very small party, so using this is probably overfitting\n",
    "\n",
    "Number_of_differnt_parties_voted_for - looks like it does matter, at least for values greater than 4. The qeustion is, do people who voted for a large number of parties tend to vote more for a certain party (party 1 got a very high value with people who voted for 7 different parties) or maybe the histogram looks like this simply because less people have higher values (let's print the histogram), but it doesn't help us predict who will they vote to.\n",
    "Since the outstanding numbers in the histograms (when exist) are for small parties, and the histograms are very different between the high numbers (the parties with more votes are not consistent) we believe it's just noise.\n",
    "\n",
    "Num_of_kids_born_last_10_years - same\n",
    "\n",
    "Last_school_grades - looks like it doesn't matter when 60 or above, we should probably reduce all these values to one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'Number_of_differnt_parties_voted_for'\n",
    "df.Number_of_differnt_parties_voted_for.plot(kind='hist')\n",
    "plt.show()\n",
    "\n",
    "print 'Num_of_kids_born_last_10_years'\n",
    "df.Num_of_kids_born_last_10_years.plot(kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like we were right, and those features are not important, they weren't included anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_features_by_type(_df):\n",
    "    def array_diff(a, b):\n",
    "        b = set(b)\n",
    "        return [aa for aa in a if aa not in b]\n",
    "    \n",
    "    all_features = [c for c in _df.columns if c!='Vote']\n",
    "    discrete_features = [c for c in _df.columns if len(_df[c].unique())<=20 and c!='Vote']\n",
    "    continuous_features = array_diff(all_features, discrete_features)\n",
    "    return [all_features, discrete_features, continuous_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mark_negative_values_as_nan(_df):\n",
    "    print 'mark_negative_values_as_nan'\n",
    "    positive_features = ['Avg_monthly_expense_when_under_age_21', 'AVG_lottary_expanses', 'Avg_Residancy_Altitude']\n",
    "    print 'Before:'\n",
    "    print _df[positive_features].min()\n",
    "\n",
    "    for f in positive_features:\n",
    "        _df[f] = _df[f].map(lambda x: x if x >= 0 else np.nan)\n",
    "\n",
    "    print '\\nAfter:'\n",
    "    print _df[positive_features].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outlier_detection(_df):\n",
    "    \"\"\"\n",
    "    for all continuous features: keep only values that are within +3 to -3 standard deviations, otherwise set nan\n",
    "    \"\"\"\n",
    "    \n",
    "    for f in CONTINUOUS_FEATURES:\n",
    "        std = _df[f].std()\n",
    "        mean = _df[f].mean()\n",
    "        _df[f] = _df[f].map(lambda x: x if np.abs(x-mean)<=(3*std) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_values_in_linear_depended_features(_df):\n",
    "    \"\"\"\n",
    "    Avg_Residancy_Altitude and Avg_monthly_expense_when_under_age_21 are linear depended in each other.\n",
    "    We can use that in order to filling the missing values in each other\n",
    "    \"\"\"\n",
    "    \n",
    "    _df.plot(x='Avg_Residancy_Altitude', y='Avg_monthly_expense_when_under_age_21')\n",
    "\n",
    "    xValues = _df['Avg_Residancy_Altitude'].values\n",
    "    yValues = _df['Avg_monthly_expense_when_under_age_21'].values\n",
    "    m = (yValues[0] - yValues[1]) / (xValues[0] - xValues[1])\n",
    "    const = yValues[0] - (m * xValues[0])\n",
    "\n",
    "    def get_avg_monthly_expense_when_under_age_21(avg_Residancy_Altitude_value):\n",
    "        return const + (m * avg_Residancy_Altitude_value)\n",
    "\n",
    "    def fill_avg_monthly_expense_when_under_age_21(row):\n",
    "        if row['Avg_Residancy_Altitude'] >=0:\n",
    "            row['Avg_monthly_expense_when_under_age_21'] = get_avg_monthly_expense_when_under_age_21(row['Avg_Residancy_Altitude'])\n",
    "        return row\n",
    "\n",
    "    _df = _df.apply(fill_avg_monthly_expense_when_under_age_21, axis=1)\n",
    "\n",
    "    # TODO: fill Avg_Residancy_Altitude by Avg_monthly_expense_when_under_age_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def categorical_features_tranformation(_df):\n",
    "\n",
    "    # Identify which of the orginal features are objects\n",
    "    ObjFeat=_df.keys()[_df.dtypes.map(lambda x: x=='object')]\n",
    "\n",
    "    # Transform the original features to categorical\n",
    "    for f in ObjFeat:\n",
    "        _df[f] = _df[f].astype(\"category\")\n",
    "        _df[f+\"Int\"] = _df[f].cat.rename_categories(range(_df[f].nunique())).astype(int)\n",
    "        _df.loc[_df[f].isnull(), f+\"Int\"] = np.nan #fix NaN conversion\n",
    "        _df[f]=_df[f+\"Int\"]\n",
    "        del _df[f+\"Int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def fill_missing_values_with_em(_df=np.nan):\n",
    "    \"\"\"\n",
    "    We assume that the data are missing completely at random\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "def fill_missing_values_with_naive_imputer(_df):\n",
    "    # for discrete features we will use 'most_frequent' strategy\n",
    "    imp_discrete = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "    _df[DISCRETE_FEATURES] = imp_discrete.fit_transform(_df[DISCRETE_FEATURES].values)\n",
    "    \n",
    "    # for continuous features we will use 'mean' strategy\n",
    "    imp_continuous = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    _df[CONTINUOUS_FEATURES] = imp_continuous.fit_transform(_df[CONTINUOUS_FEATURES].values)\n",
    "    \n",
    "def drop_missing_values(_df):\n",
    "    _df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def z_score_scaling(_df):\n",
    "    scaler = preprocessing.StandardScaler().fit(_df[CONTINUOUS_FEATURES])\n",
    "    _df[CONTINUOUS_FEATURES] = scaler.transform(_df[CONTINUOUS_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_last_school_grades(_df):\n",
    "    _df['Last_school_grades'] = _df['Last_school_grades'].map(lambda x: 60 if x >= 60 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def chi2_filter(_df):\n",
    "    alpha = 0.05\n",
    "    X = _df.drop(['Vote'], axis=1).values\n",
    "    Y = _df.Vote.values\n",
    "    v=chi2(X, Y)[1]\n",
    "    i=0\n",
    "    for c in ALL_FEATURES:\n",
    "        if c in DISCRETE_FEATURES:\n",
    "            print  str(v[i]) +\": \" + c\n",
    "            if v[i]<alpha:\n",
    "                FEATURES_TO_KEEP.append(c)            \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "def anova_filter(_df):\n",
    "    alpha = 0.05\n",
    "    X = _df.drop(['Vote'], axis=1).values\n",
    "    Y = _df.Vote.values\n",
    "    v=sklearn.feature_selection.f_classif(X, Y)[1]\n",
    "    i=0\n",
    "    for c in _df.drop(['Vote'], axis=1).columns:\n",
    "        if c not in DISCRETE_FEATURES:        \n",
    "            print  str(v[i]) + \": \" + c\n",
    "            if v[i]<alpha:\n",
    "                FEATURES_TO_KEEP.append(c)            \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def wrappersTest(X, Y, kf): \n",
    "    classifiers = {\n",
    "        \"Nearest Neighbors\": KNeighborsClassifier(15),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(max_depth=5),\n",
    "        \"Perceptron\": Perceptron(n_iter=50),\n",
    "    #     \"Linear SVM OVO\": SVC(kernel=\"linear\", C=1),\n",
    "        \"Linear SVM OVR\": LinearSVC(C=1),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators = 3)\n",
    "    }\n",
    "    res = {}\n",
    "    for name, clf in classifiers.iteritems():\n",
    "        score_sum=0 \n",
    "        print 'start ' + str(name) + ' test..'\n",
    "        for k, (train_index, test_index) in enumerate(kf):            \n",
    "            clf.fit(X[train_index], Y[train_index])            \n",
    "            acc = clf.score(X[test_index],Y[test_index])\n",
    "            score_sum += acc                 \n",
    "#             print(\"[fold {0}] {1} score: {2:.5}\".format(k, name, acc))\n",
    "        print(\"{0} average score: {1:.5}\".format(name, score_sum/kf.n_folds))\n",
    "        res[name] = score_sum/kf.n_folds\n",
    "    return res\n",
    "\n",
    "def evaulate_features(_df):\n",
    "    similar_features=['Avg_monthly_expense_when_under_age_21', 'AVG_lottary_expanses', 'Garden_sqr_meter_per_person_in_residancy_area']\n",
    "    n_folds=5\n",
    "    kf = KFold(n=len(_df), n_folds=n_folds)\n",
    "    Y = _df.Vote.values\n",
    "    \n",
    "    res = {}\n",
    "    print 'Wrappers score with all selected features:'\n",
    "    res['all'] = wrappersTest(_df[FEATURES_TO_KEEP].values, Y, kf)\n",
    "    \n",
    "    print 'Wrappers score without similar_features:'\n",
    "    res['withou similar_features'] = wrappersTest(_df[FEATURES_TO_KEEP].drop(similar_features, axis=1).values, Y, kf)\n",
    "\n",
    "    for s in similar_features:   \n",
    "        print 'Wrappers score without ' + str(s) + ':'\n",
    "        res[s] = wrappersTest(_df[FEATURES_TO_KEEP].drop(s, axis=1).values, Y, kf)\n",
    "        \n",
    "    print pd.DataFrame.from_dict(res)    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SFS(df, label, classifier, max_out_size, n_folds=5):\n",
    "    kf = KFold(n=len(df), n_folds=n_folds)\n",
    "    labels = df[label].values\n",
    "    selected_features = []\n",
    "    not_selected_features = list(df.columns)\n",
    "    not_selected_features.remove(label)\n",
    "    last_score = 0\n",
    "    while len(selected_features) < max_out_size and len(not_selected_features)>0:\n",
    "        max = 0\n",
    "        for feature in not_selected_features:\n",
    "            score = get_score(df[selected_features+[feature]].values, labels, classifier, kf)\n",
    "            if score > max:\n",
    "                max=score\n",
    "                best_feature=feature\n",
    "        if max<last_score:\n",
    "            print 'no improvemant by adding any feature'\n",
    "            break\n",
    "        selected_features.append(best_feature)\n",
    "        not_selected_features.remove(best_feature)\n",
    "        last_score=max\n",
    "        print 'selected feature: ' + best_feature + ' with score ' + str(max)\n",
    "    return selected_features\n",
    "        \n",
    "def get_score(X, Y, clf, kf):\n",
    "    score_sum=0\n",
    "    for k, (train_index, test_index) in enumerate(kf):\n",
    "        clf.fit(X[train_index], Y[train_index])\n",
    "        acc = clf.score(X[test_index],Y[test_index])\n",
    "        score_sum += acc\n",
    "    return score_sum/kf.n_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection - with data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mark_negative_values_as_nan\n",
      "Before:\n",
      "Avg_monthly_expense_when_under_age_21     -394.030092\n",
      "AVG_lottary_expanses                    -81744.489527\n",
      "Avg_Residancy_Altitude                    -131.343364\n",
      "dtype: float64\n",
      "\n",
      "After:\n",
      "Avg_monthly_expense_when_under_age_21     3.988911\n",
      "AVG_lottary_expanses                     44.957092\n",
      "Avg_Residancy_Altitude                    1.329637\n",
      "dtype: float64\n",
      "0.00365911880202: Occupation_Satisfaction\n",
      "0.0: Most_Important_Issue\n",
      "0.0: Looking_at_poles_results\n",
      "6.2254237442e-147: Married\n",
      "0.994843871005: Gender\n",
      "0.509510829892: Voting_Time\n",
      "1.11145126531e-94: Will_vote_only_large_party\n",
      "0.0: Last_school_grades\n",
      "0.697201100906: Age_group\n",
      "0.998062432461: Number_of_differnt_parties_voted_for\n",
      "0.623318625509: Main_transportation\n",
      "0.140801629388: Occupation\n",
      "0.0909226329392: Num_of_kids_born_last_10_years\n",
      "0.0: Financial_agenda_matters\n",
      "1.50355485475e-228: Avg_monthly_expense_when_under_age_21\n",
      "4.70838625817e-153: AVG_lottary_expanses\n",
      "1.5483098529e-232: Avg_Residancy_Altitude\n",
      "0.0: Yearly_ExpensesK\n",
      "0.79674464126: Financial_balance_score_(0-1)\n",
      "0.593414264545: %Of_Household_Income\n",
      "0.380845805505: Avg_government_satisfaction\n",
      "0.759140853368: Avg_education_importance\n",
      "0.99928534349: Avg_environmental_importance\n",
      "0.42173659553: Avg_Satisfaction_with_previous_vote\n",
      "0.341339781531: Avg_monthly_income_all_years\n",
      "0.914758903612: %Time_invested_in_work\n",
      "0.0: Yearly_IncomeK\n",
      "0.0: Avg_monthly_expense_on_pets_or_plants\n",
      "0.0: Avg_monthly_household_cost\n",
      "0.0: Phone_minutes_10_years\n",
      "0.0: Avg_size_per_room\n",
      "0.0: Weighted_education_rank\n",
      "0.153535800023: %_satisfaction_financial_policy\n",
      "2.04759524088e-276: Garden_sqr_meter_per_person_in_residancy_area\n",
      "0.0: Political_interest_Total_Score\n",
      "0.2748260394: Number_of_valued_Kneset_members\n",
      "0.0: Overall_happiness_score\n",
      "Wrappers score with all selected features:\n",
      "start Decision Tree test..\n",
      "Decision Tree average score: 0.7017\n",
      "start Naive Bayes test..\n",
      "Naive Bayes average score: 0.5734\n",
      "start Linear SVM OVR test..\n",
      "Linear SVM OVR average score: 0.4598\n",
      "start Perceptron test..\n",
      "Perceptron average score: 0.4304\n",
      "start Random Forest test..\n",
      "Random Forest average score: 0.8241\n",
      "start Nearest Neighbors test..\n",
      "Nearest Neighbors average score: 0.6548\n",
      "Wrappers score without similar_features:\n",
      "start Decision Tree test..\n",
      "Decision Tree average score: 0.7017\n",
      "start Naive Bayes test..\n",
      "Naive Bayes average score: 0.5732\n",
      "start Linear SVM OVR test..\n",
      "Linear SVM OVR average score: 0.4975\n",
      "start Perceptron test..\n",
      "Perceptron average score: 0.4005\n",
      "start Random Forest test..\n",
      "Random Forest average score: 0.83\n",
      "start Nearest Neighbors test..\n",
      "Nearest Neighbors average score: 0.6719\n",
      "Wrappers score without Avg_monthly_expense_when_under_age_21:\n",
      "start Decision Tree test..\n",
      "Decision Tree average score: 0.7017\n",
      "start Naive Bayes test..\n",
      "Naive Bayes average score: 0.574\n",
      "start Linear SVM OVR test..\n",
      "Linear SVM OVR average score: 0.471\n",
      "start Perceptron test..\n",
      "Perceptron average score: 0.4205\n",
      "start Random Forest test..\n",
      "Random Forest average score: 0.8256\n",
      "start Nearest Neighbors test..\n",
      "Nearest Neighbors average score: 0.6602\n",
      "Wrappers score without AVG_lottary_expanses:\n",
      "start Decision Tree test..\n",
      "Decision Tree average score: 0.7018\n",
      "start Naive Bayes test..\n",
      "Naive Bayes average score: 0.5731\n",
      "start Linear SVM OVR test..\n",
      "Linear SVM OVR average score: 0.4491\n",
      "start Perceptron test..\n",
      "Perceptron average score: 0.4302\n",
      "start Random Forest test..\n",
      "Random Forest average score: 0.8269\n",
      "start Nearest Neighbors test..\n",
      "Nearest Neighbors average score: 0.6591\n",
      "Wrappers score without Garden_sqr_meter_per_person_in_residancy_area:\n",
      "start Decision Tree test..\n",
      "Decision Tree average score: 0.7016\n",
      "start Naive Bayes test..\n",
      "Naive Bayes average score: 0.5733\n",
      "start Linear SVM OVR test..\n",
      "Linear SVM OVR average score: 0.4603\n",
      "start Perceptron test..\n",
      "Perceptron average score: 0.3954\n",
      "start Random Forest test..\n",
      "Random Forest average score: 0.8261\n",
      "start Nearest Neighbors test..\n",
      "Nearest Neighbors average score: 0.6589\n",
      "                   AVG_lottary_expanses  \\\n",
      "Decision Tree                    0.7018   \n",
      "Linear SVM OVR                   0.4491   \n",
      "Naive Bayes                      0.5731   \n",
      "Nearest Neighbors                0.6591   \n",
      "Perceptron                       0.4302   \n",
      "Random Forest                    0.8269   \n",
      "\n",
      "                   Avg_monthly_expense_when_under_age_21  \\\n",
      "Decision Tree                                     0.7017   \n",
      "Linear SVM OVR                                    0.4710   \n",
      "Naive Bayes                                       0.5740   \n",
      "Nearest Neighbors                                 0.6602   \n",
      "Perceptron                                        0.4205   \n",
      "Random Forest                                     0.8256   \n",
      "\n",
      "                   Garden_sqr_meter_per_person_in_residancy_area     all  \\\n",
      "Decision Tree                                             0.7016  0.7017   \n",
      "Linear SVM OVR                                            0.4603  0.4598   \n",
      "Naive Bayes                                               0.5733  0.5734   \n",
      "Nearest Neighbors                                         0.6589  0.6548   \n",
      "Perceptron                                                0.3954  0.4304   \n",
      "Random Forest                                             0.8261  0.8241   \n",
      "\n",
      "                   withou similar_features  \n",
      "Decision Tree                       0.7017  \n",
      "Linear SVM OVR                      0.4975  \n",
      "Naive Bayes                         0.5732  \n",
      "Nearest Neighbors                   0.6719  \n",
      "Perceptron                          0.4005  \n",
      "Random Forest                       0.8300  \n",
      "selected feature: Last_school_grades with score 0.3621\n",
      "selected feature: Overall_happiness_score with score 0.4259\n",
      "selected feature: Yearly_ExpensesK with score 0.7434\n",
      "selected feature: Weighted_education_rank with score 0.824\n",
      "selected feature: Will_vote_only_large_party with score 0.8317\n",
      "selected feature: Financial_agenda_matters with score 0.8395\n",
      "selected feature: Avg_monthly_household_cost with score 0.8421\n",
      "no improvemant by adding any feature\n",
      "features in sfs we didn't select:\n",
      "\n",
      "features we selected and sfs didn't:\n",
      "Occupation_Satisfaction\n",
      "Most_Important_Issue\n",
      "Looking_at_poles_results\n",
      "Married\n",
      "Avg_monthly_expense_when_under_age_21\n",
      "AVG_lottary_expanses\n",
      "Avg_Residancy_Altitude\n",
      "Yearly_IncomeK\n",
      "Avg_monthly_expense_on_pets_or_plants\n",
      "Phone_minutes_10_years\n",
      "Avg_size_per_room\n",
      "Garden_sqr_meter_per_person_in_residancy_area\n",
      "Political_interest_Total_Score\n",
      "['Occupation_Satisfaction', 'Most_Important_Issue', 'Looking_at_poles_results', 'Married', 'Will_vote_only_large_party', 'Last_school_grades', 'Financial_agenda_matters', 'Avg_monthly_expense_when_under_age_21', 'AVG_lottary_expanses', 'Yearly_ExpensesK', 'Yearly_IncomeK', 'Avg_monthly_expense_on_pets_or_plants', 'Avg_monthly_household_cost', 'Phone_minutes_10_years', 'Avg_size_per_room', 'Weighted_education_rank', 'Garden_sqr_meter_per_person_in_residancy_area', 'Political_interest_Total_Score', 'Overall_happiness_score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./dataset/ElectionsData.csv')\n",
    "\n",
    "ALL_FEATURES, DISCRETE_FEATURES, CONTINUOUS_FEATURES = split_features_by_type(df)\n",
    "FEATURES_TO_KEEP=[]\n",
    "\n",
    "mark_negative_values_as_nan(df)\n",
    "outlier_detection(df)\n",
    "fill_missing_values_in_linear_depended_features(df)\n",
    "categorical_features_tranformation(df)\n",
    "fill_missing_values_with_naive_imputer(df)\n",
    "reduce_last_school_grades(df)\n",
    "chi2_filter(df)\n",
    "z_score_scaling(df)\n",
    "anova_filter(df)\n",
    "evaulate_features(df)\n",
    "\n",
    "sfs=SFS(df, 'Vote', RandomForestClassifier(n_estimators = 3), 18)\n",
    "print \"features in sfs we didn't select:\"\n",
    "for f in sfs:\n",
    "    if f not in FEATURES_TO_KEEP:\n",
    "        print f\n",
    "print ''\n",
    "print \"features we selected and sfs didn't:\"\n",
    "for f in FEATURES_TO_KEEP:\n",
    "    if f not in sfs:\n",
    "        print f\n",
    "\n",
    "# as we saw - Avg_Residancy_Altitude and Avg_monthly_expense_when_under_age_21 are linearly dependant\n",
    "FEATURES_TO_KEEP.remove('Avg_Residancy_Altitude')\n",
    "\n",
    "# if we run out test on df_ we will get that 'Occupation_Satisfaction' is part of the selected features\n",
    "print FEATURES_TO_KEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection - with naive data preparation (drop na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mark_negative_values_as_nan\n",
      "Before:\n",
      "Avg_monthly_expense_when_under_age_21     -394.030092\n",
      "AVG_lottary_expanses                    -81744.489527\n",
      "Avg_Residancy_Altitude                    -131.343364\n",
      "dtype: float64\n",
      "\n",
      "After:\n",
      "Avg_monthly_expense_when_under_age_21     3.988911\n",
      "AVG_lottary_expanses                     44.957092\n",
      "Avg_Residancy_Altitude                    1.329637\n",
      "dtype: float64\n",
      "0.00939338014421: Occupation_Satisfaction\n",
      "0.0: Most_Important_Issue\n",
      "0.0: Looking_at_poles_results\n",
      "2.11157371268e-116: Married\n",
      "0.979486951977: Gender\n",
      "0.543189363847: Voting_Time\n",
      "2.5239068613e-76: Will_vote_only_large_party\n",
      "0.0: Last_school_grades\n",
      "0.628214501735: Age_group\n",
      "0.998027503891: Number_of_differnt_parties_voted_for\n",
      "0.25193533582: Main_transportation\n",
      "0.0657004618364: Occupation\n",
      "0.0908765513724: Num_of_kids_born_last_10_years\n",
      "0.0: Financial_agenda_matters\n",
      "0.0: Avg_monthly_expense_when_under_age_21\n",
      "0.0: AVG_lottary_expanses\n",
      "0.0: Avg_Residancy_Altitude\n",
      "0.0: Yearly_ExpensesK\n",
      "0.845465407459: Financial_balance_score_(0-1)\n",
      "0.321166174073: %Of_Household_Income\n",
      "0.862921643812: Avg_government_satisfaction\n",
      "0.658406948155: Avg_education_importance\n",
      "0.907063729355: Avg_environmental_importance\n",
      "0.463997124685: Avg_Satisfaction_with_previous_vote\n",
      "0.307998178073: Avg_monthly_income_all_years\n",
      "0.956600303899: %Time_invested_in_work\n",
      "0.0: Yearly_IncomeK\n",
      "0.0: Avg_monthly_expense_on_pets_or_plants\n",
      "0.0: Avg_monthly_household_cost\n",
      "0.0: Phone_minutes_10_years\n",
      "0.0: Avg_size_per_room\n",
      "0.0: Weighted_education_rank\n",
      "0.153553760405: %_satisfaction_financial_policy\n",
      "0.0: Garden_sqr_meter_per_person_in_residancy_area\n",
      "0.0: Political_interest_Total_Score\n",
      "0.150051985529: Number_of_valued_Kneset_members\n",
      "0.0: Overall_happiness_score\n",
      "selected feature: Most_Important_Issue with score 0.360543673525\n",
      "selected feature: Overall_happiness_score with score 0.431989439869\n",
      "selected feature: Phone_minutes_10_years with score 0.733245357346\n",
      "selected feature: Yearly_IncomeK with score 0.808863128797\n",
      "selected feature: Financial_agenda_matters with score 0.829364697375\n",
      "selected feature: Looking_at_poles_results with score 0.839676417088\n",
      "no improvemant by adding any feature\n",
      "features in sfs we didn't select:\n",
      "\n",
      "features we selected and sfs didn't:\n",
      "Occupation_Satisfaction\n",
      "Married\n",
      "Will_vote_only_large_party\n",
      "Last_school_grades\n",
      "Avg_monthly_expense_when_under_age_21\n",
      "AVG_lottary_expanses\n",
      "Avg_Residancy_Altitude\n",
      "Yearly_ExpensesK\n",
      "Avg_monthly_expense_on_pets_or_plants\n",
      "Avg_monthly_household_cost\n",
      "Avg_size_per_room\n",
      "Weighted_education_rank\n",
      "Garden_sqr_meter_per_person_in_residancy_area\n",
      "Political_interest_Total_Score\n",
      "['Occupation_Satisfaction', 'Most_Important_Issue', 'Looking_at_poles_results', 'Married', 'Will_vote_only_large_party', 'Last_school_grades', 'Financial_agenda_matters', 'Avg_monthly_expense_when_under_age_21', 'AVG_lottary_expanses', 'Yearly_ExpensesK', 'Yearly_IncomeK', 'Avg_monthly_expense_on_pets_or_plants', 'Avg_monthly_household_cost', 'Phone_minutes_10_years', 'Avg_size_per_room', 'Weighted_education_rank', 'Garden_sqr_meter_per_person_in_residancy_area', 'Political_interest_Total_Score', 'Overall_happiness_score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./dataset/ElectionsData.csv')\n",
    "\n",
    "ALL_FEATURES, DISCRETE_FEATURES, CONTINUOUS_FEATURES = split_features_by_type(df)\n",
    "FEATURES_TO_KEEP=[]\n",
    "\n",
    "mark_negative_values_as_nan(df)\n",
    "# outlier_detection(df)\n",
    "# fill_missing_values_in_linear_depended_features(df)\n",
    "categorical_features_tranformation(df)\n",
    "# fill_missing_values_with_naive_imputer(df)\n",
    "drop_missing_values(df)\n",
    "# reduce_last_school_grades(df)\n",
    "chi2_filter(df)\n",
    "z_score_scaling(df)\n",
    "anova_filter(df)\n",
    "evaulate_features(df)\n",
    "\n",
    "sfs=SFS(df, 'Vote', RandomForestClassifier(n_estimators = 3), 18)\n",
    "print \"features in sfs we didn't select:\"\n",
    "for f in sfs:\n",
    "    if f not in FEATURES_TO_KEEP:\n",
    "        print f\n",
    "print ''\n",
    "print \"features we selected and sfs didn't:\"\n",
    "for f in FEATURES_TO_KEEP:\n",
    "    if f not in sfs:\n",
    "        print f\n",
    "\n",
    "# as we saw - Avg_Residancy_Altitude and Avg_monthly_expense_when_under_age_21 are linearly dependant\n",
    "FEATURES_TO_KEEP.remove('Avg_Residancy_Altitude')\n",
    "\n",
    "# if we run out test on df_ we will get that 'Occupation_Satisfaction' is part of the selected features\n",
    "print FEATURES_TO_KEEP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
